import asyncio
import aiohttp
import feedparser
import trafilatura
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
import logging
import time
import sys
import os
import requests
from google import genai
from datetime import datetime
from google.genai import types



# =========================================================
# [ì„¤ì •] ê¸€ë¡œë²Œ ë³€ìˆ˜
# =========================================================
# ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„)
TIME_LIMIT_HOURS = 0.5

# í•œêµ­ ì‹œê°„(KST) ì •ì˜
KST = timezone(timedelta(hours=9))

# Trafilatura ë¡œê¹… ë„ê¸° (ì½˜ì†” ì§€ì €ë¶„í•¨ ë°©ì§€)
logging.getLogger('trafilatura').setLevel(logging.CRITICAL)

# ë¸Œë¼ìš°ì € ìœ„ì¥ìš© í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.google.com/"
}

# RSS ì£¼ì†Œ ëª©ë¡
RSS_FEEDS = [
    {"name": "í•œê²½-ì¦ê¶Œ", "url": "https://www.hankyung.com/feed/finance"},
    {"name": "í•œê²½-ê²½ì œ", "url": "https://www.hankyung.com/feed/economy"},
    {"name": "í•œê²½-ë¶€ë™ì‚°", "url": "https://www.hankyung.com/feed/realestate"},
    {"name": "í•œê²½-êµ­ì œ", "url": "https://www.hankyung.com/feed/international"},
    {"name": "í•œê²½-ì˜¤í”¼ë‹ˆì–¸", "url": "https://www.hankyung.com/feed/opinion"},
    {"name": "ë§¤ê²½-ê²½ì œ", "url": "https://www.mk.co.kr/rss/30100041/"},
    {"name": "ë§¤ê²½-êµ­ì œ", "url": "https://www.mk.co.kr/rss/30300018/"},
    {"name": "ë§¤ê²½-ê¸°ì—…ê²½ì˜", "url": "https://www.mk.co.kr/rss/50100032/"},
    {"name": "ë§¤ê²½-ì¦ê¶Œ", "url": "https://www.mk.co.kr/rss/50200011/"},
    {"name": "ë§¤ê²½-ë¶€ë™ì‚°", "url": "https://www.mk.co.kr/rss/50300009/"},
]

# =========================================================
# [ê³µí†µ] ë¹„ë™ê¸° HTTP ìš”ì²­ í—¬í¼
# =========================================================

async def fetch_html(session, url, encoding=None):
    """
    ë¹„ë™ê¸°ë¡œ URLì˜ HTML í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        async with session.get(url, headers=HEADERS, timeout=10) as response:
            if response.status == 200:
                if encoding:
                    return await response.text(encoding=encoding, errors='replace')
                else:
                    # ì¸ì½”ë”© ìë™ ê°ì§€ ì‹œë„ (utf-8 ìš°ì„ )
                    return await response.text(errors='replace')
    except Exception:
        pass
    return None

# =========================================================
# 1. RSS ì²˜ë¦¬ ë¡œì§
# =========================================================

async def get_content_smartly(session, entry):
    """RSS í•­ëª©ì—ì„œ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    html = await fetch_html(session, entry.link)
    if html:
        try:
            extracted = trafilatura.extract(html)
            if extracted and len(extracted) > 50:
                return extracted
        except: pass
        
    if hasattr(entry, 'summary'):
        try:
            summary_extract = trafilatura.extract(entry.summary)
            return f"[ìš”ì•½] {summary_extract if summary_extract else entry.summary}"
        except:
            return f"[ìš”ì•½] {entry.summary}"
            
    return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

async def process_rss_feed(session, feed_info, check_start_time):
    source_name = feed_info["name"]
    print(f"ğŸ“¡ [Start] RSS ê²€ìƒ‰ ì¤‘: {source_name}") # ì§„í–‰ ìƒí™© í‘œì‹œ
    
    results = []
    try:
        xml_data = await fetch_html(session, feed_info["url"])
        if not xml_data: return []

        feed = feedparser.parse(xml_data)
        
        # ìµœì‹ ê¸€ë¶€í„° í™•ì¸í•˜ê¸° ìœ„í•´ ì •ë ¬ ì‹œë„ (ë³´í†µ RSSëŠ” ìµœì‹ ìˆœì„)
        entries = feed.entries
        
        for entry in entries:
            if not hasattr(entry, 'published_parsed') or not entry.published_parsed:
                continue
                
            pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            
            # ì‹œê°„ í•„í„°ë§
            if pub_dt > check_start_time:
                content = await get_content_smartly(session, entry)
                results.append({
                    'source': source_name,
                    'title': entry.title,
                    'link': entry.link,
                    'published_at': pub_dt.astimezone(KST),
                    'full_content': content
                })
    except Exception as e:
        print(f"Error processing {source_name}: {e}")
        
    return results

# =========================================================
# 2. ë§¤ê²½ ì˜¤í”¼ë‹ˆì–¸ ì²˜ë¦¬ ë¡œì§
# =========================================================

def parse_mk_date_str(date_text):
    try:
        return datetime.strptime(date_text.strip().replace('.', '-'), "%Y-%m-%d %H:%M:%S").replace(tzinfo=KST)
    except:
        return None

async def process_mk_opinion(session, check_start_time_kst):
    print(f"ğŸ“¡ [Start] ë§¤ê²½ ì˜¤í”¼ë‹ˆì–¸ ê²€ìƒ‰ ì¤‘...") # ì§„í–‰ ìƒí™© í‘œì‹œ
    results = []
    base_url = "https://www.mk.co.kr/opinion/"
    
    try:
        html = await fetch_html(session, base_url)
        if not html: return []

        soup = BeautifulSoup(html, 'html.parser')
        items = soup.select('a.news_item')

        for item in items:
            try:
                link = urljoin("https://www.mk.co.kr", item['href'])
                title_tag = item.select_one('.news_ttl')
                title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                
                # ìƒì„¸ í˜ì´ì§€ ì ‘ì†
                sub_html = await fetch_html(session, link)
                if not sub_html: continue
                sub_soup = BeautifulSoup(sub_html, 'html.parser')
                
                date_area = sub_soup.select_one('.registration dd') or sub_soup.select_one('.news_input_time')
                if not date_area: continue
                
                article_dt = parse_mk_date_str(date_area.text)
                if not article_dt or article_dt <= check_start_time_kst: continue

                content = trafilatura.extract(sub_html)
                if not content:
                    body = sub_soup.select_one('.news_cnt_detail_wrap')
                    content = body.text.strip() if body else "ë³¸ë¬¸ ì‹¤íŒ¨"

                results.append({
                    'source': 'ë§¤ê²½-ì˜¤í”¼ë‹ˆì–¸',
                    'title': title,
                    'link': link,
                    'published_at': article_dt,
                    'full_content': content
                })
            except: continue
    except Exception as e:
        print(f"MK Opinion Error: {e}")
        
    return results

# =========================================================
# 3. ë”ë²¨(The Bell) ì²˜ë¦¬ ë¡œì§ (ê³ ì† ë¹„ë™ê¸° ë²„ì „)
# =========================================================

def parse_thebell_date(date_str):
    """ë”ë²¨ ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
    try:
        parts = date_str.strip().split()
        if len(parts) != 3: return None
        date_part, ampm, time_part = parts
        hour, minute, second = map(int, time_part.split(':'))
        if ampm == "ì˜¤í›„" and hour != 12: hour += 12
        elif ampm == "ì˜¤ì „" and hour == 12: hour = 0
        dt_str = f"{date_part} {hour:02d}:{minute:02d}:{second:02d}"
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=KST)
    except Exception:
        return None

async def process_thebell_article(session, item, base_url, check_start_time_kst):
    """ê°œë³„ ë”ë²¨ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‘ì—… ë‹¨ìœ„"""
    try:
        # 1. ë‚ ì§œ í™•ì¸ (ëª©ë¡ì—ì„œ)
        date_tag = item.select_one('.userBox .date')
        if not date_tag: return None
        
        date_str = date_tag.text.strip()
        article_dt = parse_thebell_date(date_str)
        if not article_dt: return None
        
        # ì‹œê°„ í•„í„°ë§
        if article_dt <= check_start_time_kst: return None
        
        # 2. ì •ë³´ ì¶”ì¶œ
        a_tag = item.select_one('dl > a')
        if not a_tag: return None
        
        relative_link = a_tag['href']
        link = urljoin(base_url, relative_link)
        title = a_tag.select_one('dt').text.strip()
        summary = a_tag.select_one('dd').text.strip()

        # 3. ìƒì„¸ í˜ì´ì§€ ë¹„ë™ê¸° ì ‘ì†
        full_html = await fetch_html(session, link, encoding='utf-8')
        full_content = ""
        
        if full_html:
            full_content = trafilatura.extract(full_html) or ""
            # Fallback
            if len(full_content) < 50:
                try:
                    art_soup = BeautifulSoup(full_html, 'html.parser')
                    content_div = art_soup.select_one('.viewSection')
                    if content_div: full_content = content_div.text.strip()
                except: pass
        
        final_content = full_content if len(full_content) > 50 else summary
        
        # ë¶ˆí•„ìš” ë¬¸êµ¬ ì œê±°
        cleanup_marker = "ì €ì‘ê¶Œì â“’ ìë³¸ì‹œì¥ ë¯¸ë””ì–´ 'thebell'"
        if cleanup_marker in final_content:
            final_content = final_content.split(cleanup_marker)[0].strip()
            
        return {
            'source': 'ë”ë²¨(The Bell)',
            'title': title,
            'link': link,
            'full_content': final_content,
            'published_at': article_dt
        }
    except:
        return None

async def get_thebell_news_async(session, check_start_time_kst):
    print(f"ğŸ“¡ [Start] ë”ë²¨(The Bell) ê²€ìƒ‰ ì¤‘...") # ì§„í–‰ ìƒí™© í‘œì‹œ
    base_url = "https://www.thebell.co.kr/free/content/article.asp?svccode=00"
    
    try:
        html = await fetch_html(session, base_url, encoding='utf-8')
        if not html: return []
        
        soup = BeautifulSoup(html, 'html.parser')
        items = soup.select('.listBox > ul > li')
        
        # ê° ê¸°ì‚¬ë¥¼ ë¹„ë™ê¸° Taskë¡œ ìƒì„±í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
        tasks = [process_thebell_article(session, item, base_url, check_start_time_kst) for item in items]
        
        # ëª¨ë“  Task ë™ì‹œ ì‹¤í–‰
        results = await asyncio.gather(*tasks)
        
        # None ì œì™¸ (ë‚ ì§œ ì§€ë‚œ ê²ƒë“¤)
        valid_results = [r for r in results if r is not None]
        return valid_results
        
    except Exception as e:
        print(f"The Bell Error: {e}")
        return []

# =========================================================
# 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (í†µí•©)
# =========================================================

async def main():
    start_time = time.time()
    
    now_utc = datetime.now(timezone.utc)
    now_kst = datetime.now(KST)
    
    # ì²´í¬ ê¸°ì¤€ ì‹œê°„ ì„¤ì •
    check_time_utc = now_utc - timedelta(hours=TIME_LIMIT_HOURS)
    check_time_kst = now_kst - timedelta(hours=TIME_LIMIT_HOURS)

    print(f"ğŸš€ [{now_kst.strftime('%Y-%m-%d %H:%M:%S')}] ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {TIME_LIMIT_HOURS}ì‹œê°„)\n")

    async with aiohttp.ClientSession() as session:
        tasks = []
        
        # 1. RSS í”¼ë“œ íƒœìŠ¤í¬ ì¶”ê°€
        for feed in RSS_FEEDS:
            tasks.append(process_rss_feed(session, feed, check_time_utc))
            
        # 2. ë§¤ê²½ ì˜¤í”¼ë‹ˆì–¸ íƒœìŠ¤í¬ ì¶”ê°€
        tasks.append(process_mk_opinion(session, check_time_kst))
        
        # 3. ë”ë²¨ íƒœìŠ¤í¬ ì¶”ê°€ (ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ë³€ê²½ë¨!)
        tasks.append(get_thebell_news_async(session, check_time_kst))

        # ëª¨ë“  íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰ ë° ëŒ€ê¸°
        all_results_grouped = await asyncio.gather(*tasks)

    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”
    flat_news_list = [news for group in all_results_grouped for news in group]
    
    # ìµœì‹ ìˆœ ì •ë ¬
    flat_news_list.sort(key=lambda x: x['published_at'], reverse=True)

    end_time = time.time()
    
    print(f"\nâœ… [Complete] ëª¨ë“  ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"ğŸ“Š ì´ {len(flat_news_list)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")
    print(f"{'='*60}\n")

    return flat_news_list

    # ê²°ê³¼ ì¶œë ¥
    # for news in flat_news_list:
    #     pub_str = news['published_at'].strftime('%H:%M')
    #     content_len = len(news.get('full_content', ''))
        
    #     print(f"[{news['source']} | {pub_str}] {news['title']}")
    #     print(f"ğŸ”— ë§í¬: {news['link']}")
    #     print(f"ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {content_len:,}ì")  # ê¸€ììˆ˜ë§Œ ì¶œë ¥
    #     print(news.get('full_content', '')[:500])
    #     print("-" * 40)

    # if len(flat_news_list) == 0:
    #     print(">>> ì§€ì •ëœ ì‹œê°„ ë‚´ì— ì—…ë°ì´íŠ¸ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # print(flat_news_list)

# =========================================================
# 5. ì‹¤í–‰ í™˜ê²½ í˜¸í™˜ì„± ì½”ë“œ
# =========================================================
if __name__ == "__main__":
    # ìœˆë„ìš° í™˜ê²½ asyncio ì •ì±… ì„¤ì •
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # Jupyter/Colab í™˜ê²½ ëŒ€ì‘
    if 'ipykernel' in sys.modules or 'google.colab' in sys.modules:
        try:
            import nest_asyncio
            nest_asyncio.apply()
            articles=asyncio.run(main())
        except ImportError:
            # nest_asyncioê°€ ì—†ìœ¼ë©´ awaitë¡œ ì‹¤í–‰ (Jupyter êµ¬ë²„ì „ ë“±)
            loop = asyncio.get_event_loop()
            loop.run_until_complete(main())
    else:
        # ì¼ë°˜ íŒŒì´ì¬ ì‹¤í–‰
        articles=asyncio.run(main())


# 1. API í‚¤ ë° ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì • - ë³´ì•ˆ í•„ìˆ˜!)

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

if not GEMINI_API_KEY or not TELEGRAM_BOT_TOKEN:
    print("Error: API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# Gemini í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = genai.Client(api_key=GEMINI_API_KEY)


model = "gemini-flash-latest"


ins="""
ë‹¹ì‹ ì€ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ì˜ ìœ ëŠ¥í•œ í€ë“œë§¤ë‹ˆì €ì…ë‹ˆë‹¤. 
â€‹[ì§€ì‹œë¬¸]
ì´ ê¸°ì‚¬ë¥¼ ì•„ë˜ì˜ [ì‘ì„± ì›ì¹™]ì— ë”°ë¼ ìš”ì•½í•´ ì£¼ì„¸ìš”.
â€‹[ì‘ì„± ì›ì¹™]
â€‹í˜•ì‹: ì„œìˆ í˜• ì¤„ê¸€ ëŒ€ì‹ , í•µì‹¬ ë‚´ìš© 5~7ê°œë¥¼ ì¶”ë ¤ ê°œì¡°ì‹ìœ¼ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
â€‹êµ¬ì¡°: ê° ë¬¸ì¥ ì•ì— [ì£¼ì œ í‚¤ì›Œë“œ]ë¥¼ ë‹¬ì•„ ë‚´ìš©ì„ ì§ê´€ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³ , ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ë‚˜ì—´í•˜ì„¸ìš”.
â€‹ê°„ê²°ì„±: ì¡°ì‚¬ì™€ ë¯¸ì‚¬ì—¬êµ¬ëŠ” ë°°ì œí•˜ê³ , 'ëª…ì‚¬í˜•' ë˜ëŠ” 'ê°œì¡°ì‹ ì–´ë¯¸'ë¡œ ê°„ê²°í•˜ê²Œ ëë§ºìœ¼ì„¸ìš”. ë¬¸ì¥ í˜¸í¡ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ëŠì–´ì£¼ì„¸ìš”.
â€‹ë°ì´í„° í™œìš© (ì¤‘ìš”): ì¶”ìƒì ì¸ í‘œí˜„(ì˜ˆ: "ëŒ€í­ ìƒìŠ¹") ëŒ€ì‹  **êµ¬ì²´ì ì¸ ìˆ˜ì¹˜(%, ê¸ˆì•¡, ê¸°ê°„ ë“±)**ë¥¼ í¬í•¨í•˜ì—¬ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.
â€‹ìš©ì–´ ì‚¬ìš©: ê²½ì œ/ì •ì¹˜/ì‹œì‚¬ ë¶„ì•¼ì˜ **í†µìš© ì•½ì–´(YoY, QoQ, BP, YTD ë“±)**ì™€ **ì „ë¬¸ ìš©ì–´(ë§¤íŒŒ/ë¹„ë‘˜ê¸°íŒŒ, ìˆì»¤ë²„ë§, í€ë”ë©˜í„¸ ë“±)**ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì˜ ì •ë³´ ë°€ë„ë¥¼ ë†’ì´ì„¸ìš”.
ê¸´ ì„œìˆ ë³´ë‹¤ëŠ” ê±´ì¡°í•œ(Dry) í†¤ì„ ìœ ì§€í•˜ì‹œê³ , í•¨ì¶•ì ì¸ í•œìì–´(ì˜ˆ: ìƒìŠ¹í•˜ë‹¤â†’ìƒíšŒ, ì§€ì¼œë³´ë‹¤â†’ê´€ë§, ê±±ì •í•˜ë‹¤â†’ìš°ë ¤)ë¥¼ ì ê·¹ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ê¸¸ì´ë¥¼ ì••ì¶•í•˜ì„¸ìš”.
ìƒìŠ¹/í•˜ë½/ë³´í•© ë“±ì˜ ë°©í–¥ì„±ì€ í…ìŠ¤íŠ¸ ëŒ€ì‹  **íŠ¹ìˆ˜ê¸°í˜¸(â†‘, â†“, -)**ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ì§ê´€ì„±ì„ ë†’ì´ì„¸ìš”.
Markdown íƒœê·¸(**, ## ë“±)ëŠ” ì¼ì ˆ ì‚¬ìš©í•˜ì§€ ë§ê³  í…ìŠ¤íŠ¸ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(thinking_budget=0),
    system_instruction=ins
)


# 2. í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ í•¨ìˆ˜
def send_telegram_message(token, chat_id, text):
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    
    # parse_modeë¥¼ ì‚­ì œí•˜ì—¬ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ AIê°€ ì–´ë–¤ íŠ¹ìˆ˜ë¬¸ìë¥¼ ë±‰ì–´ë„ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    payload = {
        "chat_id": chat_id,
        "text": text,
        "disable_web_page_preview": True,
        # "parse_mode":"markdown",
    }
    
    try:
        response = requests.post(url, json=payload)
        
        # ë§Œì•½ ë˜ ì—ëŸ¬ê°€ ë‚œë‹¤ë©´, í…”ë ˆê·¸ë¨ì´ ë³´ë‚´ì¤€ êµ¬ì²´ì ì¸ ì´ìœ ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        if response.status_code != 200:
            print(f"ì „ì†¡ ì‹¤íŒ¨ ì›ì¸: {response.text}")
            
        response.raise_for_status()
        return True
    except Exception as e:
        print(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")
        return False

# ... (ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼) ...


# 3. Gemini ìš”ì•½ í•¨ìˆ˜
def summarize_text(full_text):
    prompt = f"""
    [ê¸°ì‚¬ ë³¸ë¬¸]
    {full_text}
    """
    try:
        response = client.models.generate_content(
            model="gemini-flash-latest", contents=prompt, config=config,
        )
        return response.text

    except Exception as e:
        print(f"Gemini ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


# ==========================================
# 4. ë©”ì¸ ë¡œì§ ì‹¤í–‰
# ==========================================

print(f"ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

for article in articles:
    title = article.get('title', 'ì œëª© ì—†ìŒ')
    link = article.get('link', '')
    content = article.get('full_content', '')
    
    if not content:
        continue # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
    
    raw_date = article.get('published_at')
    date_str = ""
    
    if isinstance(raw_date, datetime):
        # ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: 2025ë…„ 12ì›” 06ì¼ 00:36)
        date_str = raw_date.strftime('%Yë…„ %mì›” %dì¼ %H:%M')
    else:
        # ë‚ ì§œ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ë‹¤ë¥¼ ê²½ìš° ëŒ€ë¹„
        date_str = str(raw_date) if raw_date else ""

    if not content:
        continue 
    
    # 1) Geminiì—ê²Œ ìš”ì•½ ìš”ì²­
    print(f"'{title}' ìš”ì•½ ì¤‘...")
    summary = summarize_text(content)

    # 2) í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í¬ë§·íŒ…
    # [ìˆ˜ì •] ì œëª© ì•„ë˜ì— ë‚ ì§œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
    message = f"[{article['source']}] {title}\n"
    if date_str:
        message += f"ğŸ“… {date_str}\n\n" # ë‚ ì§œ ì¶œë ¥
    else:
        message += "\n"
        
    message += f"{summary}\n\n"
    message += f"{link}"
    
    # 3) í…”ë ˆê·¸ë¨ ì „ì†¡
    send_telegram_message(TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID, message)


print("ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
